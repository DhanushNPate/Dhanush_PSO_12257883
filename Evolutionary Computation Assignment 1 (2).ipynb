{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b4de81-3d21-4f82-aebb-6be62e6065a7",
   "metadata": {},
   "source": [
    "**Section\tA:\tProblem\tStatement\t–\tEnhancing\tNeural\tNetwork\tPerformance\twith Particle\tSwarm\tOptimization**\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc5e524-23ed-4d6c-9343-39d4d1273661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I coose Predicting\tsecondary\tschool\tstudent\tperformance\tusing\ta\tdouble\tparticle\tswarm optimization-based\tcategorical\tboosting\tmodel\n",
    "# link - (https://www.sciencedirect.com/science/article/abs/pii/S0952197623008333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a04725ae-4827-4bca-acac-af18dd1adc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "# Use Python's requests Module to download the file directly in Jupyter Notebook\n",
    "import requests\n",
    "\n",
    "# URL of the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip\"\n",
    "\n",
    "# Send HTTP request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Save the content to a zip file\n",
    "with open(\"student-performance.zip\", \"wb\") as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "print(\"Download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcc24cbc-3233-44ed-ac96-2b3020fa1b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete!\n"
     ]
    }
   ],
   "source": [
    "# Extract the ZIP File\n",
    "import zipfile\n",
    "\n",
    "# Extract the dataset\n",
    "with zipfile.ZipFile(\"student-performance.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"student_performance\")\n",
    "\n",
    "print(\"Extraction complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a434d41-c976-45ac-9723-a1583c6217b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>school</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>famsize</th>\n",
       "      <th>Pstatus</th>\n",
       "      <th>Medu</th>\n",
       "      <th>Fedu</th>\n",
       "      <th>Mjob</th>\n",
       "      <th>Fjob</th>\n",
       "      <th>...</th>\n",
       "      <th>famrel</th>\n",
       "      <th>freetime</th>\n",
       "      <th>goout</th>\n",
       "      <th>Dalc</th>\n",
       "      <th>Walc</th>\n",
       "      <th>health</th>\n",
       "      <th>absences</th>\n",
       "      <th>G1</th>\n",
       "      <th>G2</th>\n",
       "      <th>G3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>at_home</td>\n",
       "      <td>teacher</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>LE3</td>\n",
       "      <td>T</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>at_home</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>15</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>health</td>\n",
       "      <td>services</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GP</td>\n",
       "      <td>F</td>\n",
       "      <td>16</td>\n",
       "      <td>U</td>\n",
       "      <td>GT3</td>\n",
       "      <td>T</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  ...  \\\n",
       "0     GP   F   18       U     GT3       A     4     4  at_home   teacher  ...   \n",
       "1     GP   F   17       U     GT3       T     1     1  at_home     other  ...   \n",
       "2     GP   F   15       U     LE3       T     1     1  at_home     other  ...   \n",
       "3     GP   F   15       U     GT3       T     4     2   health  services  ...   \n",
       "4     GP   F   16       U     GT3       T     3     3    other     other  ...   \n",
       "\n",
       "  famrel freetime  goout  Dalc  Walc health absences  G1  G2  G3  \n",
       "0      4        3      4     1     1      3        6   5   6   6  \n",
       "1      5        3      3     1     1      3        4   5   5   6  \n",
       "2      4        3      2     2     3      3       10   7   8  10  \n",
       "3      3        2      2     1     1      5        2  15  14  15  \n",
       "4      4        3      2     1     2      5        4   6  10  10  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Load the Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the student performance dataset\n",
    "df = pd.read_csv(\"student_performance/student-mat.csv\", sep=\";\")\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d73f635-2eee-4930-96fd-8284d2033130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyswarm in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyswarm) (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyswarm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16150b07-e5f3-49c9-b3b7-f12e0a10d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyswarm import pso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20618656-448b-4fea-8cc8-2bf60f0a73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pyswarm import pso  # PSO optimization library\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc590c69-6b4e-45f9-8a4b-d1e875927926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSO-NN Model for Hyperparameter Optimization\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.neural_network import MLPClassifier  # Import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset (modify based on your actual data)\n",
    "df = pd.read_csv(\"student_performance/student-mat.csv\", sep=\";\")\n",
    "df_encoded = pd.get_dummies(df, drop_first=True)  # One-hot encoding categorical columns\n",
    "\n",
    "X = df_encoded.drop(\"G3\", axis=1)  # Features (all except target column 'G3')\n",
    "y = df_encoded[\"G3\"]  # Target variable (final grade 'G3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2f5c27a-adaf-40f5-bcc3-896c29ce0e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "361e21b7-ff43-4f06-81af-8c9956b832a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSO Configuration\n",
    "SWARM_SIZE = 10\n",
    "DIMENSIONS = 2  # [learning rate, hidden layer size]\n",
    "NUM_GENERATIONS = 20\n",
    "W = 0.729  # Inertia weight\n",
    "C1 = 1.49  # Cognitive weight\n",
    "C2 = 1.49  # Social weight\n",
    "MIN_BOUNDARY = [0.0001, 5]  # Minimum learning rate, minimum hidden neurons\n",
    "MAX_BOUNDARY = [0.1, 100]  # Maximum learning rate, maximum hidden neurons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "191debe2-8b42-477a-9fb7-c9797929d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitness function\n",
    "def fitness_function(position):\n",
    "    lr = position[0]\n",
    "    hidden = int(position[1])  # Convert to integer for hidden layer size\n",
    "    if hidden <= 0:\n",
    "        hidden = 1  # Ensure hidden layer size is positive\n",
    "\n",
    "    model = MLPClassifier(hidden_layer_sizes=(hidden,), learning_rate_init=lr, max_iter=200, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate performance on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)  # Accuracy as fitness function\n",
    "    return 1 - acc  # We want to minimize error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7904edd7-6b9a-4c16-899a-f7be54b368ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particle Class (for PSO)\n",
    "class Particle:\n",
    "    def __init__(self):\n",
    "        self.position = [\n",
    "            random.uniform(MIN_BOUNDARY[0], MAX_BOUNDARY[0]),\n",
    "            random.uniform(MIN_BOUNDARY[1], MAX_BOUNDARY[1])\n",
    "        ]\n",
    "        self.velocity = [random.uniform(-1, 1) for _ in range(DIMENSIONS)]\n",
    "        self.fitness = fitness_function(self.position)\n",
    "        self.best_position = list(self.position)\n",
    "        self.best_fitness = self.fitness\n",
    "        self.informants = random.sample(range(SWARM_SIZE), 3)\n",
    "        self.group_best_position = list(self.position)\n",
    "        self.group_best_fitness = self.fitness\n",
    "\n",
    "    def update_velocity(self):\n",
    "        for d in range(DIMENSIONS):\n",
    "            r1, r2 = random.random(), random.random()\n",
    "            cognitive = C1 * r1 * (self.best_position[d] - self.position[d])\n",
    "            social = C2 * r2 * (self.group_best_position[d] - self.position[d])\n",
    "            self.velocity[d] = W * self.velocity[d] + cognitive + social\n",
    "\n",
    "    def update_position(self):\n",
    "        for d in range(DIMENSIONS):\n",
    "            self.position[d] += self.velocity[d]\n",
    "            self.position[d] = max(MIN_BOUNDARY[d], min(MAX_BOUNDARY[d], self.position[d]))\n",
    "        self.fitness = fitness_function(self.position)\n",
    "\n",
    "    def update_group_best(self, swarm):\n",
    "        best_informant = min(self.informants, key=lambda i: swarm[i].best_fitness)\n",
    "        if swarm[best_informant].best_fitness < self.group_best_fitness:\n",
    "            self.group_best_fitness = swarm[best_informant].best_fitness\n",
    "            self.group_best_position = list(swarm[best_informant].best_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fe20b42-62a1-4867-a7f9-27469f63def5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1: Best Accuracy = 0.2658\n",
      "Generation 2: Best Accuracy = 0.3165\n",
      "Generation 3: Best Accuracy = 0.3165\n",
      "Generation 4: Best Accuracy = 0.3165\n",
      "Generation 5: Best Accuracy = 0.3165\n",
      "Generation 6: Best Accuracy = 0.3165\n",
      "Generation 7: Best Accuracy = 0.3165\n",
      "Generation 8: Best Accuracy = 0.3165\n",
      "Generation 9: Best Accuracy = 0.3165\n",
      "Generation 10: Best Accuracy = 0.3165\n",
      "Generation 11: Best Accuracy = 0.3165\n",
      "Generation 12: Best Accuracy = 0.3165\n",
      "Generation 13: Best Accuracy = 0.3165\n",
      "Generation 14: Best Accuracy = 0.3165\n",
      "Generation 15: Best Accuracy = 0.3165\n",
      "Generation 16: Best Accuracy = 0.3165\n",
      "Generation 17: Best Accuracy = 0.3165\n",
      "Generation 18: Best Accuracy = 0.3165\n",
      "Generation 19: Best Accuracy = 0.3165\n",
      "Generation 20: Best Accuracy = 0.3165\n",
      "\n",
      "PSO Optimization Complete!\n",
      "Best Learning Rate: 0.10000\n",
      "Best Hidden Neurons: 95\n"
     ]
    }
   ],
   "source": [
    "# PSO main loop\n",
    "swarm = [Particle() for _ in range(SWARM_SIZE)]\n",
    "global_best = min(swarm, key=lambda p: p.best_fitness)\n",
    "global_best_position = list(global_best.best_position)\n",
    "global_best_fitness = global_best.best_fitness\n",
    "\n",
    "for gen in range(NUM_GENERATIONS):\n",
    "    for particle in swarm:\n",
    "        particle.update_group_best(swarm)\n",
    "        particle.update_velocity()\n",
    "        particle.update_position()\n",
    "        if particle.fitness < particle.best_fitness:\n",
    "            particle.best_fitness = particle.fitness\n",
    "            particle.best_position = list(particle.position)\n",
    "\n",
    "    best_particle = min(swarm, key=lambda p: p.best_fitness)\n",
    "    if best_particle.best_fitness < global_best_fitness:\n",
    "        global_best_fitness = best_particle.best_fitness\n",
    "        global_best_position = list(best_particle.best_position)\n",
    "\n",
    "    print(f\"Generation {gen + 1}: Best Accuracy = {1 - global_best_fitness:.4f}\")\n",
    "\n",
    "print(\"\\nPSO Optimization Complete!\")\n",
    "print(f\"Best Learning Rate: {global_best_position[0]:.5f}\")\n",
    "print(f\"Best Hidden Neurons: {int(global_best_position[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb02715d-4da5-4bee-b3fc-af3772c793f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditionally Optimized Neural Network using Grid Search\n",
    "\n",
    "#import library\n",
    "\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9942c909-0cd9-467c-a373-23370ea372fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for grid search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'max_iter': [200]\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "mlp = MLPClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "045c675f-9797-43cc-9cf3-3996e7123219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='accuracy')\n",
    "start_time = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search_time = time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2fd00fa-fcb9-4fed-8b90-dd4c0dba40e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters from Grid Search: {'hidden_layer_sizes': (100,), 'learning_rate_init': 0.01, 'max_iter': 200}\n",
      "Best Accuracy from Grid Search: 0.24365079365079367\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters and the corresponding accuracy\n",
    "print(\"Best parameters from Grid Search:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy from Grid Search:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31a7f959-fff9-4a17-8e9c-c61207d00258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy from Grid Search: 0.2658\n",
      "Grid Search Time: 11.23 seconds\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "best_mlp = grid_search.best_estimator_\n",
    "y_pred_grid = best_mlp.predict(X_test)\n",
    "grid_accuracy = accuracy_score(y_test, y_pred_grid)\n",
    "print(f\"Test Accuracy from Grid Search: {grid_accuracy:.4f}\")\n",
    "print(f\"Grid Search Time: {grid_search_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2bc8916-a128-4b0f-af09-792c9aeaaff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Results Comparison ---\n",
      "PSO-NN Test Accuracy: 0.3165\n",
      "Grid Search Test Accuracy: 0.2658\n",
      "PSO-NN Time: 7.62 seconds\n",
      "Grid Search Time: 11.23 seconds\n"
     ]
    }
   ],
   "source": [
    "# Final Evaluation\n",
    "\n",
    "# Display comparison results\n",
    "print(\"\\n--- Final Results Comparison ---\")\n",
    "print(f\"PSO-NN Test Accuracy: {1 - global_best_fitness:.4f}\")\n",
    "print(f\"Grid Search Test Accuracy: {grid_accuracy:.4f}\")\n",
    "print(f\"PSO-NN Time: {sum([particle.fitness for particle in swarm]):.2f} seconds\")\n",
    "print(f\"Grid Search Time: {grid_search_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2549fed6-ffca-435f-9bf4-32980d3c8118",
   "metadata": {},
   "source": [
    "**Section\tB:\tTask\tDescription**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "550f8317-4558-4886-afc4-fbf0cccc15e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem Selection\n",
    "#For this task, I choose text classification in Natural Language Processing (NLP) as the problem domain. Specifically, I classify news articles from the 20 Newsgroups dataset into one of 20 categories.\n",
    "\n",
    "# Objectives:\n",
    "# Build a neural network to classify the articles.\n",
    "\n",
    "# Compare the performance of a traditionally optimized NN model (using Grid Search) and a PSO-optimized NN model.\n",
    "\n",
    "# Success Metrics:\n",
    "#Accuracy of the model on the test set.\n",
    "# Time taken for training and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "805ad840-a835-4667-a3c0-ab0cc7a06fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e140d247-2024-49fa-8ebc-0d42116a7909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: pyswarm in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (71.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.5.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.8.1)\n",
      "Requirement already satisfied: namex in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install Required Libraries\n",
    "\n",
    "!pip install scikit-learn numpy tensorflow pyswarm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79aa4245-a44e-4b45-b43b-7ff2ce5d00a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess the Dataset\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "X, y = newsgroups.data, newsgroups.target\n",
    "\n",
    "# Convert text to numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fc98795-11ff-4aeb-a8ec-560d9e7638e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Baseline Neural Network Accuracy: 0.8834\n"
     ]
    }
   ],
   "source": [
    "# Build a Simple Neural Network\n",
    "\n",
    "def create_nn(learning_rate=0.01, hidden_neurons=100):\n",
    "    \"\"\"Creates and compiles a simple neural network.\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(hidden_neurons, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(len(set(y)), activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train and evaluate the traditional model\n",
    "baseline_model = create_nn()\n",
    "baseline_model.fit(X_train, y_train, epochs#=5, batch_size=32, verbose=0)\n",
    "y_pred_baseline = np.argmax(baseline_model.predict(X_test), axis=1)\n",
    "\n",
    "# Calculate baseline accuracy\n",
    "baseline_acc = accuracy_score(y_test, y_pred_baseline)\n",
    "print(f\"Baseline Neural Network Accuracy: {baseline_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4d0e6b0-329e-41c5-b8a4-551ba74cc7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Best Hyperparameters: {'hidden_neurons': 150, 'learning_rate': 0.001}\n",
      "Best Accuracy: 0.8925\n"
     ]
    }
   ],
   "source": [
    "#  to Add Grid Search for Hyperparameter Optimization\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],  # Different learning rates to try\n",
    "    'hidden_neurons': [50, 100, 150]      # Different number of neurons\n",
    "}\n",
    "\n",
    "# Iterate over all parameter combinations\n",
    "best_acc = 0\n",
    "best_params = None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    model = create_nn(learning_rate=params['learning_rate'], hidden_neurons=params['hidden_neurons'])\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
    "    \n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best Accuracy: {best_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1a65abb-60e6-4deb-87c8-ca1d5a9ae9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Stopping search: maximum iterations reached --> 3\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "PSO-Optimized Neural Network Accuracy: 0.8796\n",
      "Best Hyperparameters: Learning Rate=0.01351, Layers=1, Neurons=200, Activation=sigmoid\n"
     ]
    }
   ],
   "source": [
    "# Optimize Using PSO\n",
    "\n",
    "from pyswarm import pso\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define available activation functions\n",
    "activation_functions = ['relu', 'tanh', 'sigmoid']\n",
    "\n",
    "def create_nn(learning_rate, num_layers, neurons_per_layer, activation_index):\n",
    "    \"\"\"Creates and compiles a neural network with given hyperparameters.\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add first hidden layer\n",
    "    model.add(Dense(int(neurons_per_layer), activation=activation_functions[int(activation_index)], input_shape=(X_train.shape[1],)))\n",
    "    \n",
    "    # Add additional layers based on num_layers\n",
    "    for _ in range(int(num_layers) - 1):\n",
    "        model.add(Dense(int(neurons_per_layer), activation=activation_functions[int(activation_index)]))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(len(set(y)), activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define the fitness function for PSO\n",
    "def fitness_function(params):\n",
    "    learning_rate, num_layers, neurons_per_layer, activation_index = params\n",
    "    num_layers = int(num_layers)\n",
    "    neurons_per_layer = int(neurons_per_layer)\n",
    "    activation_index = int(activation_index)\n",
    "    \n",
    "    model = create_nn(learning_rate, num_layers, neurons_per_layer, activation_index)\n",
    "    model.fit(X_train, y_train, epochs=3, batch_size=32, verbose=0)  # Train for fewer epochs\n",
    "    \n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    return 1 - accuracy_score(y_test, y_pred)  # Minimize error\n",
    "\n",
    "# Define search space (Lower Bound and Upper Bound)\n",
    "lb = [0.0001, 1, 10, 0]  # Lower bounds (learning rate, layers, neurons, activation function index)\n",
    "ub = [0.1, 3, 200, 2]    # Upper bounds (activation function index 0=relu, 1=tanh, 2=sigmoid)\n",
    "\n",
    "# Run PSO optimization\n",
    "best_params, _ = pso(fitness_function, lb, ub, swarmsize=5, maxiter=3)  # Fewer particles, fewer iterations\n",
    "\n",
    "# Train the optimized model\n",
    "optimized_model = create_nn(best_params[0], int(best_params[1]), int(best_params[2]), int(best_params[3]))\n",
    "optimized_model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
    "y_pred_pso = np.argmax(optimized_model.predict(X_test), axis=1)\n",
    "\n",
    "# Calculate PSO-optimized accuracy\n",
    "pso_acc = accuracy_score(y_test, y_pred_pso)\n",
    "print(f\"PSO-Optimized Neural Network Accuracy: {pso_acc:.4f}\")\n",
    "print(f\"Best Hyperparameters: Learning Rate={best_params[0]:.5f}, Layers={int(best_params[1])}, Neurons={int(best_params[2])}, Activation={activation_functions[int(best_params[3])]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48652a36-7095-4b46-8683-63817d3eaf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting statsmodels\n",
      "  Downloading statsmodels-0.14.4-cp312-cp312-win_amd64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsmodels) (1.26.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsmodels) (1.14.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsmodels) (2.2.2)\n",
      "Collecting patsy>=0.5.6 (from statsmodels)\n",
      "  Downloading patsy-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from statsmodels) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\patel\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.16.0)\n",
      "Downloading statsmodels-0.14.4-cp312-cp312-win_amd64.whl (9.8 MB)\n",
      "   ---------------------------------------- 0.0/9.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.8 MB 330.3 kB/s eta 0:00:30\n",
      "    --------------------------------------- 0.2/9.8 MB 1.2 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.4/9.8 MB 2.2 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.7/9.8 MB 3.0 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.8/9.8 MB 3.3 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.1/9.8 MB 3.6 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.4/9.8 MB 3.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.7/9.8 MB 4.1 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.9/9.8 MB 4.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.2/9.8 MB 4.4 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.4/9.8 MB 4.4 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.7/9.8 MB 4.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.9/9.8 MB 4.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.3/9.8 MB 4.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.5/9.8 MB 4.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.7/9.8 MB 4.7 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.9/9.8 MB 4.7 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.2/9.8 MB 4.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.5/9.8 MB 4.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.8/9.8 MB 5.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.1/9.8 MB 5.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.2/9.8 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.6/9.8 MB 5.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.8/9.8 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.1/9.8 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.3/9.8 MB 5.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.6/9.8 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.9/9.8 MB 5.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.2/9.8 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.5/9.8 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.8/9.8 MB 5.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.1/9.8 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.4/9.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/9.8 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.0/9.8 MB 5.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.3/9.8 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.6/9.8 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.8/9.8 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.8/9.8 MB 5.3 MB/s eta 0:00:00\n",
      "Downloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n",
      "   ---------------------------------------- 0.0/232.9 kB ? eta -:--:--\n",
      "   -------------------------------------- - 225.3/232.9 kB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 232.9/232.9 kB 4.7 MB/s eta 0:00:00\n",
      "Installing collected packages: patsy, statsmodels\n",
      "Successfully installed patsy-1.0.1 statsmodels-0.14.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9b3d0fb-0d43-4848-a3e8-8ba75c015b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Traditional vs PSO-Optimized Model:\n",
      "Baseline Model Accuracy: 0.8834\n",
      "PSO-Optimized Model Accuracy: 0.8796\n",
      "\n",
      "Statistical Test (McNemar's Test):\n",
      "p-value: 0.21425\n",
      "No **statistically significant** improvement from PSO optimization.\n"
     ]
    }
   ],
   "source": [
    "# Compare Results\n",
    "\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import numpy as np\n",
    "\n",
    "# Generate confusion matrix for McNemar's test\n",
    "contingency_table = np.zeros((2, 2))\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    baseline_correct = (y_pred_baseline[i] == y_test[i])\n",
    "    pso_correct = (y_pred_pso[i] == y_test[i])\n",
    "    \n",
    "    if baseline_correct and pso_correct:\n",
    "        contingency_table[0, 0] += 1  # Both correct\n",
    "    elif baseline_correct and not pso_correct:\n",
    "        contingency_table[0, 1] += 1  # Baseline correct, PSO wrong\n",
    "    elif not baseline_correct and pso_correct:\n",
    "        contingency_table[1, 0] += 1  # Baseline wrong, PSO correct\n",
    "    else:\n",
    "        contingency_table[1, 1] += 1  # Both wrong\n",
    "\n",
    "# Apply McNemar's test\n",
    "result = mcnemar(contingency_table, exact=True)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nComparison of Traditional vs PSO-Optimized Model:\")\n",
    "print(f\"Baseline Model Accuracy: {baseline_acc:.4f}\")\n",
    "print(f\"PSO-Optimized Model Accuracy: {pso_acc:.4f}\")\n",
    "\n",
    "# Check statistical significance\n",
    "print(\"\\nStatistical Test (McNemar's Test):\")\n",
    "print(f\"p-value: {result.pvalue:.5f}\")\n",
    "if result.pvalue < 0.05:\n",
    "    print(\"The improvement from PSO optimization is **statistically significant**.\")\n",
    "else:\n",
    "    print(\"No **statistically significant** improvement from PSO optimization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e921bf9a-d5e8-4c36-ab55-d803c546c418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
